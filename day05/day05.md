Voici une proposition de leÃ§on complÃ¨te pour **S5 â€” RÃ©seau : lire des URL & parser HTML (stdlib)**.
Jâ€™ai gardÃ© le style des semaines prÃ©cÃ©dentes : intro thÃ©orique, exemples concrets, puis exercices et fil rouge.

---

# ğŸ“¡ S5 â€” RÃ©seau : lire des URL & parser HTML (stdlib)

Jusquâ€™ici, tu as travaillÃ© sur des fichiers locaux. Cette fois-ci, on passe au **web** : lire des pages et en extraire de lâ€™information. Pas besoin de bibliothÃ¨ques externes comme `requests` ou `BeautifulSoup` â€” on reste dans la **stdlib** de Python.

---

## 1. Lire une page web

On utilise `urllib.request` :

```python
from urllib.request import urlopen

url = "https://www.example.com/"
with urlopen(url, timeout=5) as response:
    html_bytes = response.read()  # contenu brut
    encoding = response.headers.get_content_charset() or "utf-8"
    html = html_bytes.decode(encoding)

print(html[:200])  # affiche les 200 premiers caractÃ¨res
```

ğŸ‘‰ Points clÃ©s :

- `timeout=5` Ã©vite que le programme reste bloquÃ©.
- Le contenu est en **bytes**, il faut dÃ©coder (`decode`) selon lâ€™encodage annoncÃ© dans lâ€™en-tÃªte HTTP.
- Si rien nâ€™est annoncÃ© : utiliser `"utf-8"` par dÃ©faut.

---

## 2. Parser du HTML minimaliste

Python propose un parseur trÃ¨s simple : `html.parser`. On peut dÃ©finir une sous-classe de `HTMLParser` pour rÃ©agir quand on rencontre certaines balises :

```python
from html.parser import HTMLParser

class MyHTMLParser(HTMLParser):
    def __init__(self):
        super().__init__()
        self.in_title = False
        self.title = None
        self.links = []

    def handle_starttag(self, tag, attrs):
        if tag == "title":
            self.in_title = True
        if tag == "a":
            for (attr, value) in attrs:
                if attr == "href":
                    self.links.append(value)

    def handle_endtag(self, tag):
        if tag == "title":
            self.in_title = False

    def handle_data(self, data):
        if self.in_title:
            self.title = (self.title or "") + data.strip()

# Exemple dâ€™utilisation :
parser = MyHTMLParser()
parser.feed("<html><head><title>Demo</title></head><body><a href='/x'>link</a></body></html>")
print(parser.title)   # Demo
print(parser.links)   # ['/x']
```

---

## 3. Mettre ensemble : URL â†’ titre + liens

```python
from urllib.request import urlopen
from html.parser import HTMLParser
import json

class PageParser(HTMLParser):
    def __init__(self):
        super().__init__()
        self.in_title = False
        self.title = None
        self.links = []

    def handle_starttag(self, tag, attrs):
        if tag == "title":
            self.in_title = True
        if tag == "a":
            hrefs = [v for (k, v) in attrs if k == "href"]
            self.links.extend(hrefs)

    def handle_endtag(self, tag):
        if tag == "title":
            self.in_title = False

    def handle_data(self, data):
        if self.in_title:
            self.title = (self.title or "") + data.strip()

def fetch_page(url: str) -> dict:
    with urlopen(url, timeout=5) as response:
        encoding = response.headers.get_content_charset() or "utf-8"
        html = response.read().decode(encoding)

    parser = PageParser()
    parser.feed(html)

    return {
        "url": url,
        "title": parser.title,
        "links": parser.links
    }

# Test :
page = fetch_page("https://www.example.com/")
print(json.dumps(page, indent=2))
```

---

## 4. Ligne de commande avec argparse

On ajoute une petite CLI pour faire Ã§a en direct :

```python
import argparse

def main():
    p = argparse.ArgumentParser(description="Fetch a web page and extract links")
    p.add_argument("--fetch-url", required=True, help="URL de la page Ã  analyser")
    p.add_argument("--out", default="page.json", help="Fichier de sortie JSON")
    args = p.parse_args()

    data = fetch_page(args.fetch_url)

    with open(args.out, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

    print(f"[OK] RÃ©sultats sauvegardÃ©s dans {args.out}")

if __name__ == "__main__":
    main()
```

ğŸ‘‰ Exemple dâ€™exÃ©cution :

```bash
python data_explorer.py --fetch-url https://www.python.org --out python.json
```

---

## 5. Exercices

1. Modifier le parseur pour aussi extraire les balises `<h1>`.
2. Compter le nombre de liens internes (`href` commenÃ§ant par `/`) vs externes (`httpâ€¦`).
3. Faire un petit crawler Ã  1 niveau : tÃ©lÃ©charger une page, puis tÃ©lÃ©charger toutes les pages internes liÃ©es depuis elle (attention Ã  ne pas aller trop loin !).

---

## ğŸ¯ Fil rouge

Le programme doit pouvoir :

- Lire une URL donnÃ©e (`--fetch-url`).
- Extraire le `title` et la liste des `href`.
- Sauvegarder dans un JSON de la forme :

```json
{
  "url": "https://www.example.com/",
  "title": "Example Domain",
  "links": ["https://www.iana.org/domains/example"]
}
```
